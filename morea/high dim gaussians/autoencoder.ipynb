{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.1549096  34.16914374 30.51843851 27.54001439 24.92521973 22.5803388\n",
      " 20.31556874 18.25926376 16.55469272 15.00953804 13.4580718  12.13379529\n",
      " 10.98172736  9.97753199  8.979406    8.1138124   7.29755663  6.56452899\n",
      "  5.8796556   5.33594489  4.81722402  4.36209592  3.94931645  3.55532657\n",
      "  3.19966893  2.89028877  2.61533515  2.36179739  2.14271402  1.93660099\n",
      "  1.74294661  1.58034335  1.41908014  1.27270045  1.15077674  1.0380125\n",
      "  0.9380639   0.84339567  0.76028914  0.69031817  0.61881157  0.5601978\n",
      "  0.49892636  0.44779954  0.40178157  0.35889015  0.32562174  0.29148827\n",
      "  0.26012688  0.23308897]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(0,1,(1000,50))\n",
    "import numpy.linalg as lin\n",
    "\n",
    "# the following is to make X more skewed towards a low rank matrix, so the results\n",
    "# are more instructive. \n",
    "u,l,v = lin.svd(X)\n",
    "wt = [1.1**(-x) for x in range(len(l))]\n",
    "l = l * wt\n",
    "print(l)\n",
    "\n",
    "X = u @ np.vstack((np.diag(l),np.zeros((950,50)))) @ v.T\n",
    "\n",
    "# m is the number of coordinates of the input examples, 10 is the \n",
    "# number of latent coordinates we will use to reconstruct the data\n",
    "m = 50\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 322us/step - loss: 0.1923\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.1460\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 393us/step - loss: 0.1206\n",
      "Epoch 4/20\n",
      " 256/1000 [======>.......................] - ETA: 0s - loss: 0.1071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prasadsn\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.107438). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 745us/step - loss: 0.1020\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 293us/step - loss: 0.0860\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0721\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 86us/step - loss: 0.0607\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 0.0521\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 118us/step - loss: 0.0456\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 146us/step - loss: 0.0406\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 145us/step - loss: 0.0367\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 0.0334\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 0.0308\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 249us/step - loss: 0.0286\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 198us/step - loss: 0.0269\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 283us/step - loss: 0.0254\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 196us/step - loss: 0.0243\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 119us/step - loss: 0.0234\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 0.0226\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 0.0221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x183dc059908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = models.Sequential()\n",
    "Wlayer = layers.Dense(k,activation='linear', input_shape=(m,))\n",
    "autoencoder.add(Wlayer)\n",
    "\n",
    "Vlayer = layers.Dense(m,activation='linear')\n",
    "# If you want to fix V and see how W varies, uncomment the following line:\n",
    "# Vlayer.trainable = False\n",
    "autoencoder.add(Vlayer)\n",
    "\n",
    "#Vlayer.set_weights([V.T,np.zeros(50,)])\n",
    "autoencoder.compile(optimizer='rmsprop', loss='mse')\n",
    "autoencoder.fit(X,X,epochs=20,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 179us/step - loss: 0.0216\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 0.0213\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 0.0210\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 98us/step - loss: 0.0208\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 160us/step - loss: 0.0206\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0204\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 159us/step - loss: 0.0203\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 89us/step - loss: 0.0202\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 218us/step - loss: 0.0202\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 99us/step - loss: 0.0201\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 1s 752us/step - loss: 0.0200\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 0.0200\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 1s 543us/step - loss: 0.0200 0s - loss: \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 150us/step - loss: 0.0199\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 178us/step - loss: 0.0199\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 0.0199\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 90us/step - loss: 0.0199\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 142us/step - loss: 0.0199\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0198\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 143us/step - loss: 0.0198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x183db83e488>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X,X,epochs=20,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of W:  (10, 50)\n",
      "shape of V:  (50, 10)\n"
     ]
    }
   ],
   "source": [
    "W = autoencoder.layers[0].get_weights()[0].T\n",
    "V = autoencoder.layers[1].get_weights()[0].T\n",
    "print('shape of W: ', W.shape)\n",
    "print('shape of V: ', V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0., -0., -0., -0.,  0.,  0.,  0., -0., -0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -0., -0.,  0., -0., -0.],\n",
       "       [-0., -0.,  1., -0., -0., -0., -0., -0., -0.,  0.],\n",
       "       [-0.,  0., -0.,  1.,  0.,  0.,  0., -0.,  0., -0.],\n",
       "       [ 0.,  0., -0., -0.,  1., -0.,  0.,  0.,  0., -0.],\n",
       "       [-0., -0., -0.,  0., -0.,  1., -0.,  0., -0., -0.],\n",
       "       [ 0.,  0., -0., -0., -0.,  0.,  1., -0., -0., -0.],\n",
       "       [ 0., -0., -0.,  0., -0.,  0.,  0.,  1.,  0., -0.],\n",
       "       [-0., -0.,  0., -0., -0., -0., -0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that WV =I, the weights W of the first \n",
    "# layer is the projection into the column space of the weights of the second layer, V.\n",
    "# You won't get exactly I because the optimization is stochastic, but close enough\n",
    "\n",
    "np.round(W @ V,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2, -0.1, -0.1, ...,  0.1,  0. , -0. ],\n",
       "       [-0. ,  0.2,  0.1, ...,  0. ,  0. ,  0. ],\n",
       "       [-0.1,  0.1,  0.1, ..., -0. , -0. ,  0.1],\n",
       "       ...,\n",
       "       [ 0. , -0.1, -0. , ...,  0.4,  0. ,  0.1],\n",
       "       [ 0. ,  0. , -0. , ..., -0.1,  0.1, -0.1],\n",
       "       [-0. ,  0. ,  0.1, ...,  0.1, -0. ,  0.2]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(V @ W, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a new model that ties the weights of the W and V layers (ie forces W=V^T) \n",
    "# to force the weight matrices V (W resp) of the prior example to have orthogonal \n",
    "# columns, ie V^T V = I (rows resp, W W^T = I) \n",
    "\n",
    "# This approximates the SVD. To tie the weights together, we will have to \n",
    "# extend the keras.layers.Layer class to take in another layer, and use the weights from \n",
    "# that layer. The minimal thing we need to do for this is redefine the output computation \n",
    "# when the layer is called, and this is achieved by redefining the inbuilt call function of \n",
    "# the Layer class. \n",
    "\n",
    "class mylayer(layers.Layer):\n",
    "    def __init__(self, tracklayer=None, activation=None, **kwargs):\n",
    "        self.kernel_to_track = tracklayer\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        output = tf.matmul(inputs, self.kernel_to_track.weights[0], transpose_b = True)\n",
    "        return self.activation(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define the autoencoder model:\n",
    "\n",
    "class autoencoder_svd(keras.Model):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = layers.Dense(10, activation = 'linear')\n",
    "        self.decoder = mylayer(self.encoder, activation = 'linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        w = self.encoder(inputs)\n",
    "        self.latent = w\n",
    "        hatx = self.decoder(w)\n",
    "        return(hatx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 152us/step - loss: 0.1432\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 0.1224\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1052\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 0.0872\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.0716\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0594\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 184us/step - loss: 0.0503\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0442\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0401\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 116us/step - loss: 0.0371\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.0347\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 169us/step - loss: 0.0327\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 0.0309\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 126us/step - loss: 0.0292\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 139us/step - loss: 0.0278\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.0265\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 123us/step - loss: 0.0254\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 355us/step - loss: 0.0245 0s - loss: 0.024 - ETA: 0s - loss: 0.02\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 1s 508us/step - loss: 0.0238 0s - loss: 0.\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 0.0232 0s - loss: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x183d33c8b08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdnn = autoencoder_svd()\n",
    "\n",
    "svdnn.compile(optimizer='adam', loss='mse')\n",
    "svdnn.fit(X,X,epochs=20,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 146us/step - loss: 0.0227\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 101us/step - loss: 0.0223\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.0220\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 118us/step - loss: 0.0217\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 122us/step - loss: 0.0215\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 132us/step - loss: 0.0213\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 151us/step - loss: 0.0212\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 116us/step - loss: 0.0211\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 0.0209 0s - loss: 0.02\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 193us/step - loss: 0.0209\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 124us/step - loss: 0.0208\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 0.0208\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 407us/step - loss: 0.0207\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 174us/step - loss: 0.0207\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 199us/step - loss: 0.0206\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 0.0206\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 149us/step - loss: 0.0206\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 104us/step - loss: 0.0206\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 486us/step - loss: 0.0206\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 414us/step - loss: 0.0206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x183dc5a2fc8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdnn.fit(X,X,epochs=20,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0., -0.,  0., -0.,  0.,  0., -0., -0.,  0.],\n",
       "       [-0.,  1., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.],\n",
       "       [-0., -0.,  1., -0., -0., -0., -0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., -0.,  1., -0.,  0., -0.,  0., -0., -0.],\n",
       "       [-0.,  0., -0., -0.,  1.,  0., -0.,  0., -0.,  0.],\n",
       "       [ 0.,  0., -0.,  0.,  0.,  1.,  0., -0.,  0., -0.],\n",
       "       [ 0., -0., -0., -0., -0.,  0.,  1., -0.,  0., -0.],\n",
       "       [-0., -0.,  0.,  0.,  0., -0., -0.,  1.,  0., -0.],\n",
       "       [-0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., -0.,  0., -0., -0., -0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = svdnn.encoder.weights[0].numpy() \n",
    "print(W.shape)\n",
    "\n",
    "np.round(W.T @W,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10)\n",
      "[0.95233522 0.99358056 0.95987988 0.9969597  0.9835332  0.99383293\n",
      " 0.99459085 0.96955484 0.98650035 0.99621449]\n"
     ]
    }
   ],
   "source": [
    "# finally, we check how much power the columns of W carry on the top 10 eigen-vectors\n",
    "# of X^TX, ie the best directions to represent the rows of X.\n",
    "# The closer these numbers are to 1, the better the column space of W represents the \n",
    "# optimal linear space to project the rows of X into. Not all numbers need to be \n",
    "# close to 1, but at least some need to be.\n",
    "\n",
    "V = v[:,0:10]\n",
    "print(V.shape)\n",
    "\n",
    "\n",
    "# This is the projection of W into smaller eigen-directions\n",
    "pwr = lin.inv(V.T @ V) @ V.T @ W\n",
    "\n",
    "print(np.diag(pwr.T @ pwr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
